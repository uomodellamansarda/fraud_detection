{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "#Importing all the module that we will use\n",
    "\n",
    "#to read csv and manipulate dataframe\n",
    "import pandas as pd \n",
    "\n",
    "#to modify array\n",
    "import numpy as np \n",
    "\n",
    "#to do some data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing the class to convert SMSs to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Note that count vectorizer just counts the word frequencies\n",
    "#TFIDF  vectorizer assigns a score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Importing the classifiers that we wil test \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Importing performance metrix to evaluate our models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,auc,roc_auc_score\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the two main function for our task\n",
    "#one for get prediction from out classifier\n",
    "#the other one to evaluate performance of the model based on different \n",
    "#key perfomance metrics\n",
    "def get_predictions(clf, count_train, y_train, count_test):\n",
    "    # create classifier\n",
    "    clf = clf\n",
    "    # fit it to training data\n",
    "    clf.fit(count_train,y_train)\n",
    "    # predict using test data\n",
    "    y_pred = clf.predict(count_test)\n",
    "    # Compute predicted probabilities: y_pred_prob\n",
    "    y_pred_prob = clf.predict_proba(count_test)\n",
    "    #for fun: train-set predictions\n",
    "    #train_pred = clf.predict(count_test)\n",
    "    #print('train-set confusion matrix:\\n', confusion_matrix(y_train,train_pred)) \n",
    "    return y_pred, y_pred_prob\n",
    "def print_scores(y_test,y_pred,y_pred_prob):\n",
    "    conf_matrix=confusion_matrix(y_test,y_pred)\n",
    "    recall= recall_score(y_test,y_pred)\n",
    "    precision= precision_score(y_test,y_pred)\n",
    "    f1=f1_score(y_test,y_pred)\n",
    "    accuracy= accuracy_score(y_test,y_pred)\n",
    "    roc= roc_auc_score(y_test, y_pred_prob[:,1])\n",
    "    print('test-set confusion matrix:\\n',conf_matrix ) \n",
    "    print(\"recall score: \",recall )\n",
    "    print(\"precision score: \", precision)\n",
    "    print(\"f1 score: \",f1 )\n",
    "    print(\"accuracy score: \",   accuracy)\n",
    "    print(\"ROC AUC: {}\".format(roc_auc_score(y_test, y_pred_prob[:,1])))\n",
    "    kpi=pd.DataFrame([[recall,precision,f1,accuracy,roc]], columns=['Recall','Precision','F1_Score','Accuracy','ROC-AUC'])\n",
    "    return kpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                SMS\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "label    5572 non-null object\n",
      "SMS      5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 130.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Name of the file in the directory\n",
    "name_path='data_spam.csv'\n",
    "#reading the csv and converting in a df\n",
    "csv_raw=pd.read_csv(name_path,index_col=0,encoding=\"ISO-8859-1\")\n",
    "#We need to inspect our dataframe \n",
    "print(csv_raw.head(2))\n",
    "print(csv_raw.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is composed by\n",
      "        SMS\n",
      "label      \n",
      "ham    4825\n",
      "spam    747\n",
      "Our Dataset as pie chart:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We count how many genuine and fraud SMS are in the dataset\n",
    "#We are going to group the dataset by label \n",
    "#and plot thorugh a pie chart\n",
    "group_label=csv_raw.groupby(by='label').count()\n",
    "print('The dataset is composed by')\n",
    "print(group_label)\n",
    "# We Plot Our Dataset\n",
    "print(\"Our Dataset as pie chart:\")\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.pie(group_label,autopct='%1.1f%%', labels=['Genuine','Fraud'], colors=['yellow','r'])\n",
    "plt.axis('equal')\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Note, if we pick a random SMS, on a frequentistic approach we have 13.4% of probability to pick a Fraudulent SMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                                SMS  Status\n",
      "0   ham  Go until jurong point, crazy.. Available only ...    True\n",
      "1   ham                      Ok lar... Joking wif u oni...    True\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   False\n",
      "3   ham  U dun say so early hor... U c already then say...    True\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...    True\n"
     ]
    }
   ],
   "source": [
    "#We need to create a dummy variable 1-0 for our dataset for faster test-training and later manipulation\n",
    "df=csv_raw.copy()\n",
    "b={'ham':True,'spam':False}\n",
    "df['Status']=df['label'].map(b)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classifier=[MultinomialNB(),LogisticRegression(),RandomForestClassifier(),RandomForestClassifier(criterion='entropy')]\n",
    "names=['MultinomialNaiveBayes','Logistic Regression', 'Random Forest Classifier', 'Random Forest Classifier crit=Entropy']\n",
    "y=df['Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['SMS'],y,test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes--> Generative classifiers learn a model of joint probabilities p(x, y) and use Bayes rule to calculate p(x\ty) to make a prediction\n",
    "Logistic Regression --> Discriminative models learn the posterior probability p(x\ty) “directly”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 225   24]\n",
      " [  11 1579]]\n",
      "recall score:  0.9930817610062893\n",
      "precision score:  0.9850280723643169\n",
      "f1 score:  0.9890385217663639\n",
      "accuracy score:  0.9809679173463839\n",
      "ROC AUC: 0.9847414816498699\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 210   39]\n",
      " [   2 1588]]\n",
      "recall score:  0.9987421383647799\n",
      "precision score:  0.9760295021511985\n",
      "f1 score:  0.9872552067143301\n",
      "accuracy score:  0.977705274605764\n",
      "ROC AUC: 0.98672930716577\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 212   37]\n",
      " [   7 1583]]\n",
      "recall score:  0.9955974842767296\n",
      "precision score:  0.9771604938271605\n",
      "f1 score:  0.9862928348909658\n",
      "accuracy score:  0.9760739532354541\n",
      "ROC AUC: 0.9797858099062918\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 201   48]\n",
      " [   6 1584]]\n",
      "recall score:  0.9962264150943396\n",
      "precision score:  0.9705882352941176\n",
      "f1 score:  0.9832402234636872\n",
      "accuracy score:  0.9706362153344209\n",
      "ROC AUC: 0.9735495440883029\n",
      "__________________\n",
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 200   49]\n",
      " [   0 1590]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9701037217815741\n",
      "f1 score:  0.9848250232270053\n",
      "accuracy score:  0.9733550842849374\n",
      "ROC AUC: 0.9582657674724052\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 132  117]\n",
      " [   0 1590]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9314586994727593\n",
      "f1 score:  0.9645131938125568\n",
      "accuracy score:  0.9363784665579119\n",
      "ROC AUC: 0.9600111136369377\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 157   92]\n",
      " [   1 1589]]\n",
      "recall score:  0.9993710691823899\n",
      "precision score:  0.9452706722189174\n",
      "f1 score:  0.9715683277285234\n",
      "accuracy score:  0.9494290375203915\n",
      "ROC AUC: 0.9276426460559218\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 157   92]\n",
      " [   0 1590]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9453032104637337\n",
      "f1 score:  0.9718826405867971\n",
      "accuracy score:  0.9499728113104948\n",
      "ROC AUC: 0.9222601096208733\n",
      "__________________\n",
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 227   22]\n",
      " [   6 1584]]\n",
      "recall score:  0.9962264150943396\n",
      "precision score:  0.9863013698630136\n",
      "f1 score:  0.9912390488110138\n",
      "accuracy score:  0.9847743338771071\n",
      "ROC AUC: 0.9812129019221542\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 209   40]\n",
      " [   2 1588]]\n",
      "recall score:  0.9987421383647799\n",
      "precision score:  0.9754299754299754\n",
      "f1 score:  0.9869484151646987\n",
      "accuracy score:  0.9771615008156607\n",
      "ROC AUC: 0.9874542193932964\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 195   54]\n",
      " [   2 1588]]\n",
      "recall score:  0.9987421383647799\n",
      "precision score:  0.9671132764920828\n",
      "f1 score:  0.9826732673267327\n",
      "accuracy score:  0.9695486677542142\n",
      "ROC AUC: 0.9724179737819201\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 199   50]\n",
      " [   5 1585]]\n",
      "recall score:  0.9968553459119497\n",
      "precision score:  0.9694189602446484\n",
      "f1 score:  0.9829457364341084\n",
      "accuracy score:  0.9700924415443176\n",
      "ROC AUC: 0.9787944229749186\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "#In this section we will use count vectorizer for the three classifiers, selecting keywords and bi-grams \n",
    "results_df=pd.DataFrame(columns=['n_grams','Classifier','Recall','Precision','F1_Score','Accuracy','ROC-AUC'])\n",
    "performance=pd.DataFrame()\n",
    "for i in [[1,1],[2,2],[1,2]]:\n",
    "    count_vectorizer=CountVectorizer(stop_words='english',ngram_range=(i))\n",
    "    count_train=count_vectorizer.fit_transform(X_train.values)\n",
    "    count_test=count_vectorizer.transform(X_test.values)\n",
    "    for name, clf in zip(names,list_classifier):\n",
    "        print('Classifier used',name)\n",
    "        print('n-gram range is',i)\n",
    "        print()\n",
    "        y_pred, y_pred_prob = get_predictions(clf, count_train, y_train, count_test)\n",
    "        n_grams=str(i)\n",
    "        classifier=name\n",
    "        loop_performance=pd.DataFrame()\n",
    "        loop_performance=print_scores(y_test,y_pred,y_pred_prob)\n",
    "        loop_performance['n_grams']=n_grams\n",
    "        loop_performance['Classifier']=name\n",
    "        performance=performance.append(loop_performance)\n",
    "        print('__________________')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Recall  Precision  F1_Score  Accuracy   ROC-AUC n_grams  \\\n",
      "0  0.996226   0.986301  0.991239  0.984774  0.981213  [1, 2]   \n",
      "0  0.993082   0.985028  0.989039  0.980968  0.984741  [1, 1]   \n",
      "0  0.998742   0.976030  0.987255  0.977705  0.986729  [1, 1]   \n",
      "0  0.998742   0.975430  0.986948  0.977162  0.987454  [1, 2]   \n",
      "0  0.995597   0.977160  0.986293  0.976074  0.979786  [1, 1]   \n",
      "0  1.000000   0.970104  0.984825  0.973355  0.958266  [2, 2]   \n",
      "0  0.996226   0.970588  0.983240  0.970636  0.973550  [1, 1]   \n",
      "0  0.996855   0.969419  0.982946  0.970092  0.978794  [1, 2]   \n",
      "0  0.998742   0.967113  0.982673  0.969549  0.972418  [1, 2]   \n",
      "0  1.000000   0.945303  0.971883  0.949973  0.922260  [2, 2]   \n",
      "0  0.999371   0.945271  0.971568  0.949429  0.927643  [2, 2]   \n",
      "0  1.000000   0.931459  0.964513  0.936378  0.960011  [2, 2]   \n",
      "\n",
      "                              Classifier  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0                    Logistic Regression  \n",
      "0                    Logistic Regression  \n",
      "0               Random Forest Classifier  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0               Random Forest Classifier  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0               Random Forest Classifier  \n",
      "0                    Logistic Regression  \n"
     ]
    }
   ],
   "source": [
    "#Now we have to tidy our output \n",
    "print(performance.sort_values(['F1_Score'],ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 184   60]\n",
      " [   0 1595]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9637462235649547\n",
      "f1 score:  0.9815384615384616\n",
      "accuracy score:  0.967373572593801\n",
      "ROC AUC: 0.9870574027442314\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 173   71]\n",
      " [   2 1593]]\n",
      "recall score:  0.9987460815047022\n",
      "precision score:  0.9573317307692307\n",
      "f1 score:  0.9776004909481435\n",
      "accuracy score:  0.9603045133224578\n",
      "ROC AUC: 0.9891926614933964\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 206   38]\n",
      " [   3 1592]]\n",
      "recall score:  0.9981191222570532\n",
      "precision score:  0.9766871165644172\n",
      "f1 score:  0.9872868217054264\n",
      "accuracy score:  0.977705274605764\n",
      "ROC AUC: 0.9783377871421963\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [1, 1]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 194   50]\n",
      " [   7 1588]]\n",
      "recall score:  0.9956112852664577\n",
      "precision score:  0.9694749694749695\n",
      "f1 score:  0.9823693164243736\n",
      "accuracy score:  0.9690048939641109\n",
      "ROC AUC: 0.9824310087877075\n",
      "__________________\n",
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 118  126]\n",
      " [   0 1595]]\n",
      "recall score:  1.0\n",
      "precision score:  0.926786751888437\n",
      "f1 score:  0.9620024125452352\n",
      "accuracy score:  0.9314845024469821\n",
      "ROC AUC: 0.9611914795210442\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[  21  223]\n",
      " [   0 1595]]\n",
      "recall score:  1.0\n",
      "precision score:  0.8773377337733773\n",
      "f1 score:  0.9346615880457075\n",
      "accuracy score:  0.8787384448069603\n",
      "ROC AUC: 0.9613636363636363\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 152   92]\n",
      " [   1 1594]]\n",
      "recall score:  0.9993730407523511\n",
      "precision score:  0.9454329774614472\n",
      "f1 score:  0.971654983236818\n",
      "accuracy score:  0.9494290375203915\n",
      "ROC AUC: 0.9390320674238142\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [2, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 141  103]\n",
      " [   0 1595]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9393404004711425\n",
      "f1 score:  0.9687215305192833\n",
      "accuracy score:  0.9439912996193583\n",
      "ROC AUC: 0.9389369957346215\n",
      "__________________\n",
      "Classifier used MultinomialNaiveBayes\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 164   80]\n",
      " [   0 1595]]\n",
      "recall score:  1.0\n",
      "precision score:  0.9522388059701492\n",
      "f1 score:  0.9755351681957186\n",
      "accuracy score:  0.9564980967917346\n",
      "ROC AUC: 0.9840664987923327\n",
      "__________________\n",
      "Classifier used Logistic Regression\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 150   94]\n",
      " [   2 1593]]\n",
      "recall score:  0.9987460815047022\n",
      "precision score:  0.944279786603438\n",
      "f1 score:  0.9707495429616088\n",
      "accuracy score:  0.9477977161500816\n",
      "ROC AUC: 0.9891438408962436\n",
      "__________________\n",
      "Classifier used Random Forest Classifier\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 194   50]\n",
      " [   9 1586]]\n",
      "recall score:  0.9943573667711598\n",
      "precision score:  0.969437652811736\n",
      "f1 score:  0.9817393995666975\n",
      "accuracy score:  0.9679173463839043\n",
      "ROC AUC: 0.978073128115525\n",
      "__________________\n",
      "Classifier used Random Forest Classifier crit=Entropy\n",
      "n-gram range is [1, 2]\n",
      "\n",
      "test-set confusion matrix:\n",
      " [[ 187   57]\n",
      " [   2 1593]]\n",
      "recall score:  0.9987460815047022\n",
      "precision score:  0.9654545454545455\n",
      "f1 score:  0.9818181818181819\n",
      "accuracy score:  0.9679173463839043\n",
      "ROC AUC: 0.9781964643609641\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['SMS'],y,test_size=0.33)\n",
    "#In this section we will use Tfidf vectorizer for the three classifiers, selecting keywords and bi-grams \n",
    "results_df=pd.DataFrame(columns=['n_grams','Classifier','Recall','Precision','F1_Score','Accuracy','ROC-AUC'])\n",
    "performance=pd.DataFrame()\n",
    "for i in [[1,1],[2,2],[1,2]]:\n",
    "    tfidf_vectorizer=TfidfVectorizer(stop_words='english',ngram_range=(i))\n",
    "    tfidf_train=tfidf_vectorizer.fit_transform(X_train.values)\n",
    "    tfidf_test=tfidf_vectorizer.transform(X_test.values)\n",
    "    for name, clf in zip(names,list_classifier):\n",
    "        print('Classifier used',name)\n",
    "        print('n-gram range is',i)\n",
    "        print()\n",
    "        y_pred, y_pred_prob = get_predictions(clf, tfidf_train, y_train, tfidf_test)\n",
    "        n_grams=str(i)\n",
    "        classifier=name\n",
    "        loop_performance=pd.DataFrame()\n",
    "        loop_performance=print_scores(y_test,y_pred,y_pred_prob)\n",
    "        loop_performance['n_grams']=n_grams\n",
    "        loop_performance['Classifier']=name\n",
    "        performance=performance.append(loop_performance)\n",
    "        print('__________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Recall  Precision  F1_Score  Accuracy   ROC-AUC n_grams  \\\n",
      "0  0.998119   0.976687  0.987287  0.977705  0.978338  [1, 1]   \n",
      "0  0.995611   0.969475  0.982369  0.969005  0.982431  [1, 1]   \n",
      "0  0.998746   0.965455  0.981818  0.967917  0.978196  [1, 2]   \n",
      "0  0.994357   0.969438  0.981739  0.967917  0.978073  [1, 2]   \n",
      "0  1.000000   0.963746  0.981538  0.967374  0.987057  [1, 1]   \n",
      "0  0.998746   0.957332  0.977600  0.960305  0.989193  [1, 1]   \n",
      "0  1.000000   0.952239  0.975535  0.956498  0.984066  [1, 2]   \n",
      "0  0.999373   0.945433  0.971655  0.949429  0.939032  [2, 2]   \n",
      "0  0.998746   0.944280  0.970750  0.947798  0.989144  [1, 2]   \n",
      "0  1.000000   0.939340  0.968722  0.943991  0.938937  [2, 2]   \n",
      "0  1.000000   0.926787  0.962002  0.931485  0.961191  [2, 2]   \n",
      "0  1.000000   0.877338  0.934662  0.878738  0.961364  [2, 2]   \n",
      "\n",
      "                              Classifier  \n",
      "0               Random Forest Classifier  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0               Random Forest Classifier  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0                    Logistic Regression  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0               Random Forest Classifier  \n",
      "0                    Logistic Regression  \n",
      "0  Random Forest Classifier crit=Entropy  \n",
      "0                  MultinomialNaiveBayes  \n",
      "0                    Logistic Regression  \n"
     ]
    }
   ],
   "source": [
    "#Now we have to tidy our output \n",
    "print(performance.sort_values(['F1_Score'],ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We didn't make in depth analysis of overfitting/underfitting, next step will use cross folds validation\n",
    "#As we saw in the pie chart, data are skewed. One test to do is to undersample the dataset \n",
    "#in order to get a less skewed dataset an analyze the results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
